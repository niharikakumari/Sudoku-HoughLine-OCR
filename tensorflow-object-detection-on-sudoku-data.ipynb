{"cells":[{"metadata":{},"cell_type":"markdown","source":"# TF Object Detection on Custom Data\nIn this notebook we will train a TensorFlow Object Detection model with a (large) custom dataset. We will cover the following steps:  \n* Install TensorFlow and TF Object Detection API\n* Fetch a pre-trained model from the TensorFlow detection model zoo\n* Configure the model and run training with the custom dataset\n* Make predictions with the trained model\n\nNow, the TensorFlow Object Detection API is not for the faint of heart to get started on, but once a few tweaks are in place, it is mostly smooth sailing."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"## Install TF Object Detection API\nThe [Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection) is at the time of writing not compatible with TF2 , so we need to install TF1.14 first. This notebook produces quite a lot of local files, and to keep a tidy house any large files not required will be removed (`rm -fr`)."},{"metadata":{"trusted":true},"cell_type":"code","source":"%cd /kaggle/working","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/working\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\nHAVE_GPU = True # change according to environment\nif HAVE_GPU:\n    !pip install --user tensorflow-gpu==1.14 -q\nelse:\n    !pip install --user tensorflow==1.14 -q\n# never mind the `ERROR: tensorflow 2.1...` message below","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make sure we the required packages\n!pip install --user Cython -q\n!pip install --user contextlib2 -q\n!pip install --user pillow -q\n!pip install --user lxml -q\n!pip install --user matplotlib -q\n\n!pip install --user lvis -q","execution_count":3,"outputs":[{"output_type":"stream","text":"\u001b[31mERROR: allennlp 0.9.0 requires flaky, which is not installed.\u001b[0m\n\u001b[31mERROR: allennlp 0.9.0 requires responses>=0.7, which is not installed.\u001b[0m\n\u001b[31mERROR: tensorflow 2.1.0rc0 has requirement tensorboard<2.1.0,>=2.0.0, but you'll have tensorboard 1.14.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: tensorflow 2.1.0rc0 has requirement tensorflow-estimator<2.1.0,>=2.0.0, but you'll have tensorflow-estimator 1.14.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: tensorflow-probability 0.8.0 has requirement cloudpickle==1.1.1, but you'll have cloudpickle 1.2.2 which is incompatible.\u001b[0m\n\u001b[31mERROR: kmeans-smote 0.1.2 has requirement imbalanced-learn<0.5,>=0.4.0, but you'll have imbalanced-learn 0.5.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: kmeans-smote 0.1.2 has requirement numpy<1.16,>=1.13, but you'll have numpy 1.19.5 which is incompatible.\u001b[0m\n\u001b[31mERROR: kmeans-smote 0.1.2 has requirement scikit-learn<0.21,>=0.19.0, but you'll have scikit-learn 0.21.3 which is incompatible.\u001b[0m\n\u001b[31mERROR: hyperopt 0.2.2 has requirement networkx==2.2, but you'll have networkx 2.4 which is incompatible.\u001b[0m\n\u001b[31mERROR: docs 0.2.2 has requirement networkx==2.2, but you'll have networkx 2.4 which is incompatible.\u001b[0m\n\u001b[31mERROR: allennlp 0.9.0 has requirement spacy<2.2,>=2.1.0, but you'll have spacy 2.2.3 which is incompatible.\u001b[0m\n\u001b[33m  WARNING: The scripts f2py, f2py3 and f2py3.6 are installed in '/root/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"We need to install the `protoc` compiler. On windows, you can get [precompiled binaries here](https://github.com/protocolbuffers/protobuf/releases)."},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget -O protobuf.zip https://github.com/google/protobuf/releases/download/v3.0.0/protoc-3.0.0-linux-x86_64.zip -q\n!unzip -o protobuf.zip\n!rm protobuf.zip","execution_count":4,"outputs":[{"output_type":"stream","text":"Archive:  protobuf.zip\r\n   creating: include/\r\n   creating: include/google/\r\n   creating: include/google/protobuf/\r\n  inflating: include/google/protobuf/struct.proto  \r\n  inflating: include/google/protobuf/type.proto  \r\n  inflating: include/google/protobuf/descriptor.proto  \r\n  inflating: include/google/protobuf/api.proto  \r\n  inflating: include/google/protobuf/empty.proto  \r\n   creating: include/google/protobuf/compiler/\r\n  inflating: include/google/protobuf/compiler/plugin.proto  \r\n  inflating: include/google/protobuf/any.proto  \r\n  inflating: include/google/protobuf/field_mask.proto  \r\n  inflating: include/google/protobuf/wrappers.proto  \r\n  inflating: include/google/protobuf/timestamp.proto  \r\n  inflating: include/google/protobuf/duration.proto  \r\n  inflating: include/google/protobuf/source_context.proto  \r\n   creating: bin/\r\n  inflating: bin/protoc              \r\n  inflating: readme.txt              \r\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Time to fetch the Object Detection API.\n<div class=\"alert alert-block alert-info\">\n<b>Tip:</b> Move up one level to avoid kernel crash when cloning repositories with deep folder structure.\n</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"%cd /kaggle\n!rm -fr models\n!git clone https://github.com/tensorflow/models.git\n!rm -fr models/.git","execution_count":5,"outputs":[{"output_type":"stream","text":"/kaggle\nCloning into 'models'...\nremote: Enumerating objects: 13, done.\u001b[K\nremote: Counting objects: 100% (13/13), done.\u001b[K\nremote: Compressing objects: 100% (12/12), done.\u001b[K\nremote: Total 50733 (delta 4), reused 10 (delta 1), pack-reused 50720\u001b[K\nReceiving objects: 100% (50733/50733), 568.65 MiB | 29.88 MiB/s, done.\nResolving deltas: 100% (34554/34554), done.\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Then compile the protocol buffer messages needed by the API."},{"metadata":{"trusted":true},"cell_type":"code","source":"# compile ProtoBuffers\n%cd models/research\n!../../working/bin/protoc object_detection/protos/*.proto --python_out=.","execution_count":6,"outputs":[{"output_type":"stream","text":"/kaggle/models/research\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\nos.environ['AUTOGRAPH_VERBOSITY'] = '0'\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nos.environ['PYTHONPATH']=os.environ['PYTHONPATH']+':/kaggle/models/research/slim:/kaggle/models/research'\nos.environ['PYTHONPATH']","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"'/kaggle/lib/kagglegym:/kaggle/lib:/kaggle/models/research/slim:/kaggle/models/research'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install tf_slim","execution_count":8,"outputs":[{"output_type":"stream","text":"Collecting tf_slim\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/97/b0f4a64df018ca018cc035d44f2ef08f91e2e8aa67271f6f19633a015ff7/tf_slim-1.1.0-py2.py3-none-any.whl (352kB)\n\u001b[K     |████████████████████████████████| 358kB 2.8MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: absl-py>=0.2.2 in /opt/conda/lib/python3.6/site-packages (from tf_slim) (0.8.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from absl-py>=0.2.2->tf_slim) (1.13.0)\nInstalling collected packages: tf-slim\nSuccessfully installed tf-slim-1.1.0\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"That's it! We can now test our setup by running `model_builder_test.py`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# !pwd\n# !python object_detection/builders/model_builder_test.py","execution_count":9,"outputs":[{"output_type":"stream","text":"/kaggle/models/research\n/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Yohoo, it works!  \nNow, we did not install the Coco API, since we will be using the Pascal VOC evaluation metric. Unfortunately, there are some hardcoded references to the Coco API that needs to be commented out. Alternatively, just install the Coco API."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --user 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI' -q","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def disable_coco(file):\n#     with open(file,'r') as f:\n#         file_str = f.read()\n#     file_str=file_str.replace('from object_detection.metrics import coco_evaluation',\n#                     '#from object_detection.metrics import coco_evaluation')\n#     file_str=file_str.replace('object_detection.metrics import coco_tools',\n#                     '#object_detection.metrics import coco_tools')\n#     file_str=file_str.replace('\\'coco_detection_metrics\\':', '#\\'coco_detection_metrics\\':')\n#     file_str=file_str.replace('coco_evaluation.CocoDetectionEvaluator,', '#coco_evaluation.CocoDetectionEvaluator,')\n#     file_str=file_str.replace('\\'coco_mask_metrics\\':','#\\'coco_mask_metrics\\':')\n#     file_str=file_str.replace('coco_evaluation.CocoMaskEvaluator,','#coco_evaluation.CocoMaskEvaluator,')\n#     with open(file,'w') as f:\n#         f.write(file_str)\n\n# disable_coco('./object_detection/eval_util.py')","execution_count":10,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fetch a model from the zoo\nWe will start with a pre-trained model from [Tensorflow detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md). Our custom dataset is the [ArTaxOr dataset](https://www.kaggle.com/mistag/arthropod-taxonomy-orders-object-detection-dataset), which contains images of invertebrate animals. Thus it makes sense to choose one of the iNaturalist Species-trained model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"%cd object_detection\n!wget -O faster_rcnn_resnet50_fgvc_2018_07_19.tar.gz http://download.tensorflow.org/models/object_detection/faster_rcnn_resnet50_fgvc_2018_07_19.tar.gz -q\n!tar xvzf faster_rcnn_resnet50_fgvc_2018_07_19.tar.gz\n!rm faster_rcnn_resnet50_fgvc_2018_07_19.tar.gz\n%cd ..","execution_count":11,"outputs":[{"output_type":"stream","text":"/kaggle/models/research/object_detection\nfaster_rcnn_resnet50_fgvc_2018_07_19/model.ckpt.meta\nfaster_rcnn_resnet50_fgvc_2018_07_19/model.ckpt.index\nfaster_rcnn_resnet50_fgvc_2018_07_19/pipeline.config\nfaster_rcnn_resnet50_fgvc_2018_07_19/saved_model/\nfaster_rcnn_resnet50_fgvc_2018_07_19/checkpoint\nfaster_rcnn_resnet50_fgvc_2018_07_19/\nfaster_rcnn_resnet50_fgvc_2018_07_19/saved_model/saved_model.pb\nfaster_rcnn_resnet50_fgvc_2018_07_19/frozen_inference_graph.pb\nfaster_rcnn_resnet50_fgvc_2018_07_19/model.ckpt.data-00000-of-00001\nfaster_rcnn_resnet50_fgvc_2018_07_19/saved_model/variables/\n/kaggle/models/research\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n<b>Tip:</b> Remove the <b>checkpoint</b> file, otherwise training will fail while loading graph.\n</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm object_detection/faster_rcnn_resnet50_fgvc_2018_07_19/checkpoint","execution_count":12,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Furthermore, create a directory for saved models (otherwise an error will occur when training is finished)."},{"metadata":{"trusted":true},"cell_type":"code","source":"%cd object_detection/faster_rcnn_resnet50_fgvc_2018_07_19\n!mkdir export\n%cd export\n!mkdir Servo\n%cd ../../..","execution_count":13,"outputs":[{"output_type":"stream","text":"/kaggle/models/research/object_detection/faster_rcnn_resnet50_fgvc_2018_07_19\n/kaggle/models/research/object_detection/faster_rcnn_resnet50_fgvc_2018_07_19/export\n/kaggle/models/research\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## **Time to create tf-records**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\nimport pandas as pd\nimport numpy as np\nimport cv2\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nwith open('/kaggle/input/sudokuimagesannotations/sudoku_annot_2.json', \"r\") as read_file:\n    data = json.load(read_file)\n    \npath = \"/kaggle/input/sudokuscreenshots/sudoku_imgs/\"\nkeys_list = list(data['_via_img_metadata'].keys())\njson_list = []\nfor key in keys_list:\n    i= keys_list.index(key)\n    val = data['_via_img_metadata'][key]['regions'][0]['shape_attributes']\n    key = key[:-6]\n    temp_image = cv2.imread(path + str(key))\n    value = (key,temp_image.shape[0],temp_image.shape[1],'Sudoku',val['x'],val['y'], val['x'] + val['width'], val['y'] + val['height'])\n    json_list.append(value)\n    \ncolumn_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\njson_df = pd.DataFrame(json_list, columns=column_name)\n\n# json_df.head()","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%cd ../../\n%cd working/\n!mkdir prog_outputs/\n%cd prog_outputs\n\njson_df.to_csv('/kaggle/working/prog_outputs/sudoku_labels.csv', index=False)\njson_df.head()","execution_count":15,"outputs":[{"output_type":"stream","text":"/kaggle\n/kaggle/working\n/kaggle/working/prog_outputs\n","name":"stdout"},{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"                                            filename  width  height   class  \\\n0  Screenshot_2021-01-07-23-19-00-858_com.easybra...   2280    1080  Sudoku   \n1  Screenshot_2021-01-07-23-19-21-187_com.easybra...   2280    1080  Sudoku   \n2  Screenshot_2021-01-07-23-19-37-128_com.easybra...   2280    1080  Sudoku   \n3  Screenshot_2021-01-07-23-19-52-081_com.easybra...   2280    1080  Sudoku   \n4  Screenshot_2021-01-07-23-20-03-976_com.easybra...   2280    1080  Sudoku   \n\n   xmin  ymin  xmax  ymax  \n0     8   306  1059  1369  \n1     5   299  1062  1365  \n2    19   323  1066  1366  \n3    13   306  1076  1373  \n4    17   310  1068  1369  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filename</th>\n      <th>width</th>\n      <th>height</th>\n      <th>class</th>\n      <th>xmin</th>\n      <th>ymin</th>\n      <th>xmax</th>\n      <th>ymax</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Screenshot_2021-01-07-23-19-00-858_com.easybra...</td>\n      <td>2280</td>\n      <td>1080</td>\n      <td>Sudoku</td>\n      <td>8</td>\n      <td>306</td>\n      <td>1059</td>\n      <td>1369</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Screenshot_2021-01-07-23-19-21-187_com.easybra...</td>\n      <td>2280</td>\n      <td>1080</td>\n      <td>Sudoku</td>\n      <td>5</td>\n      <td>299</td>\n      <td>1062</td>\n      <td>1365</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Screenshot_2021-01-07-23-19-37-128_com.easybra...</td>\n      <td>2280</td>\n      <td>1080</td>\n      <td>Sudoku</td>\n      <td>19</td>\n      <td>323</td>\n      <td>1066</td>\n      <td>1366</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Screenshot_2021-01-07-23-19-52-081_com.easybra...</td>\n      <td>2280</td>\n      <td>1080</td>\n      <td>Sudoku</td>\n      <td>13</td>\n      <td>306</td>\n      <td>1076</td>\n      <td>1373</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Screenshot_2021-01-07-23-20-03-976_com.easybra...</td>\n      <td>2280</td>\n      <td>1080</td>\n      <td>Sudoku</td>\n      <td>17</td>\n      <td>310</td>\n      <td>1068</td>\n      <td>1369</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir temp/\n%cd temp\n!cp -r '/kaggle/input/sudokuscreenshots/sudoku_imgs/' ./","execution_count":16,"outputs":[{"output_type":"stream","text":"/kaggle/working/prog_outputs/temp\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#not rescaling it now, as labelling has already been done\n\n# from PIL import Image\n# import os\n\n# def rescale_images(directory, size):\n#     for img in os.listdir(directory):\n#         im = Image.open(directory+img)\n#         im_resized = im.resize(size, Image.ANTIALIAS)\n#         im_resized.save(directory+img)\n\n# size = (512,512)\n# rescale_images ('/kaggle/working/prog_outputs/temp/sudoku_imgs/', size)","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from os import listdir\nimport random\nfrom os.path import isfile, join\nmypath = '/kaggle/working/prog_outputs/temp/sudoku_imgs/'\nonlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\nrandom.shuffle(onlyfiles)\n\ntrain_imgs = onlyfiles[:70]\ntest_imgs = onlyfiles[70:86]\nprint(len(train_imgs))\nprint(len(test_imgs))","execution_count":18,"outputs":[{"output_type":"stream","text":"70\n16\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%cd ..\n!mkdir train_imgs/\n!mkdir test_imgs/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from shutil import copyfile, copy2\ndirectory = '/kaggle/working/prog_outputs/temp/sudoku_imgs/'\nfor img in os.listdir(directory):\n    source = directory + img\n    if img in train_imgs:\n        copy2(source, 'train_imgs/')\n    elif img in test_imgs:\n        copy2(source, 'test_imgs/' )","execution_count":21,"outputs":[{"output_type":"stream","text":"/kaggle/working\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = json_df\ntest_labels = json_df\nfor index, row in json_df.iterrows():\n    if row['filename'] in train_imgs:\n        test_labels = test_labels.drop(index)\n    elif row['filename'] in test_imgs:\n        train_labels = train_labels.drop(index)\n\nprint(len(train_labels))\nprint(len(test_labels))\n\ntest_labels = test_labels.reset_index()\ntest_labels = test_labels.drop('index', axis = 1)\ntest_labels.to_csv('test_labels.csv',index=False)\n\ntrain_labels = train_labels.reset_index()\ntrain_labels = train_labels.drop('index', axis = 1)\ntrain_labels.to_csv('train_labels.csv',index=False)","execution_count":22,"outputs":[{"output_type":"stream","text":"70\n16\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"after running till here I am in this folder  /kaggle/working/prog_outputs, I should be in Kaggle/"},{"metadata":{"trusted":true},"cell_type":"code","source":"%cd ../../\n%cd models/research/","execution_count":23,"outputs":[{"output_type":"stream","text":"/\n[Errno 2] No such file or directory: 'models/research/'\n/\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nUsage:\n  # From tensorflow/models/\n  # Create train data:\n  python generate_tfrecord.py --csv_input=data/train_labels.csv  --output_path=train.record\n  # Create test data:\n  python generate_tfrecord.py --csv_input=data/test_labels.csv  --output_path=test.record\n\"\"\"\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import absolute_import\n\nimport os\nimport io\nimport pandas as pd\nimport tensorflow as tf\n\nfrom PIL import Image\nfrom object_detection.utils import dataset_util\nfrom collections import namedtuple, OrderedDict\n\n# flags = tf.app.flags\n# flags.DEFINE_string('csv_input', '', 'Path to the CSV input')\n# flags.DEFINE_string('output_path', '', 'Path to output TFRecord')\n# flags.DEFINE_string('image_dir', '', 'Path to images')\n# FLAGS = flags.FLAGS\n\n\n# TO-DO replace this with label map\ndef class_text_to_int(row_label):\n    if row_label == 'Sudoku':\n        return 1\n    else:\n        None\n\n\ndef split(df, group):\n    data = namedtuple('data', ['filename', 'object'])\n    gb = df.groupby(group)\n    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n\n\ndef create_tf_example(group, path):\n    with tf.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n        encoded_jpg = fid.read()\n    encoded_jpg_io = io.BytesIO(encoded_jpg)\n    image = Image.open(encoded_jpg_io)\n    width, height = image.size\n\n    filename = group.filename.encode('utf8')\n    image_format = b'jpg'\n    xmins = []\n    xmaxs = []\n    ymins = []\n    ymaxs = []\n    classes_text = []\n    classes = []\n\n    for index, row in group.object.iterrows():\n        xmins.append(row['xmin'] / width)\n        xmaxs.append(row['xmax'] / width)\n        ymins.append(row['ymin'] / height)\n        ymaxs.append(row['ymax'] / height)\n        classes_text.append(row['class'].encode('utf8'))\n        classes.append(class_text_to_int(row['class']))\n\n    tf_example = tf.train.Example(features=tf.train.Features(feature={\n        'image/height': dataset_util.int64_feature(height),\n        'image/width': dataset_util.int64_feature(width),\n        'image/filename': dataset_util.bytes_feature(filename),\n        'image/source_id': dataset_util.bytes_feature(filename),\n        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n        'image/format': dataset_util.bytes_feature(image_format),\n        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n        'image/object/class/label': dataset_util.int64_list_feature(classes),\n    }))\n    return tf_example\n\n\ndef main(csv_input, output_path,image_dir):\n    writer = tf.python_io.TFRecordWriter(output_path)\n#     path = os.path.join(image_dir)\n    print(path)\n    examples = pd.read_csv(csv_input)\n    grouped = split(examples, 'filename')\n    for group in grouped:\n#         print(group)\n        tf_example = create_tf_example(group, path)\n        writer.write(tf_example.SerializeToString())\n\n    writer.close()\n    output_path = os.path.join(os.getcwd(), output_path)\n    print('Successfully created the TFRecords: {}'.format(output_path))\n\n\n# if __name__ == '__main__':\n#     tf.app.run()","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%cd ../../\n%cd working/prog_outputs/\n!mkdir tf-records/\n%cd ../../","execution_count":29,"outputs":[{"output_type":"stream","text":"/kaggle\n/kaggle/working/prog_outputs\n/kaggle\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_output_path = 'working/prog_outputs/tf-records/train.record'\ntrain_csv_input = 'working/prog_outputs/train_labels.csv'\ntrain_image_dir = 'working/prog_outputs/train_imgs/'\n\ntest_output_path = 'working/prog_outputs/tf-records/test.record'\ntest_csv_input = 'working/prog_outputs/test_labels.csv'\ntest_image_dir = 'working/prog_outputs/test_imgs/'\n\nmain(train_csv_input, train_output_path, train_image_dir)\nmain(test_csv_input, test_output_path, test_image_dir)","execution_count":30,"outputs":[{"output_type":"stream","text":"/kaggle/input/sudokuscreenshots/sudoku_imgs/\nSuccessfully created the TFRecords: /kaggle/working/prog_outputs/tf-records/train.record\n/kaggle/input/sudokuscreenshots/sudoku_imgs/\nSuccessfully created the TFRecords: /kaggle/working/prog_outputs/tf-records/test.record\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pwd","execution_count":31,"outputs":[{"output_type":"execute_result","execution_count":31,"data":{"text/plain":"'/kaggle'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%cd working/prog_outputs/tf-records\n!ls -la\n%cd ../../../","execution_count":32,"outputs":[{"output_type":"stream","text":"/kaggle/working/prog_outputs/tf-records\ntotal 25060\ndrwxr-xr-x 2 root root     4096 Jan 21 18:55 .\ndrwxr-xr-x 6 root root     4096 Jan 21 18:55 ..\n-rw-r--r-- 1 root root  4655461 Jan 21 18:55 test.record\n-rw-r--r-- 1 root root 20994223 Jan 21 18:55 train.record\n/kaggle\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Create a label map .pbtxt file"},{"metadata":{"trusted":true},"cell_type":"code","source":"from object_detection.protos.string_int_label_map_pb2 import StringIntLabelMap, StringIntLabelMapItem\nfrom google.protobuf import text_format\n\n\ndef convert_classes(classes, start=1):\n    msg = StringIntLabelMap()\n    for id, name in enumerate(classes, start=start):\n        msg.item.append(StringIntLabelMapItem(id=id, name=name))\n\n    text = str(text_format.MessageToBytes(msg, as_utf8=True), 'utf-8')\n    return text\n\n\n\ntxt = convert_classes(['Sudoku'])\nprint(txt)\nwith open('working/prog_outputs/label_map.pbtxt', 'w') as f:\n    f.write(txt)","execution_count":33,"outputs":[{"output_type":"stream","text":"item {\n  name: \"Sudoku\"\n  id: 1\n}\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Config file\nThen we need to define the model `.config` file. Here we set up paths to the dataset and a few other parameters. Thankfully, TFRecords for the ArTaxOr dataset has been created in [this notebook](https://www.kaggle.com/mistag/tensorflow-tfrecords-demystified) so we can link directly to its output files. We will use a 80-20 split for training and evaluation, and since the dataset is sharded in 50 files, we can simply select 10 of them (arbitrary) to go into the evaluation set. We also need to determine how many images there are in the evaluation set to configure the evaluation stage correctly:"},{"metadata":{"trusted":true},"cell_type":"code","source":"%cd models/research/\n# !ls -la","execution_count":34,"outputs":[{"output_type":"stream","text":"/kaggle/models/research\ntotal 100\ndrwxr-xr-x 23 root root 4096 Jan 21 18:51 .\ndrwxr-xr-x  7 root root 4096 Jan 21 18:51 ..\n-rw-r--r--  1 root root 7132 Jan 21 18:51 README.md\ndrwxr-xr-x  2 root root 4096 Jan 21 18:51 a3c_blogpost\ndrwxr-xr-x  3 root root 4096 Jan 21 18:51 adversarial_text\ndrwxr-xr-x  3 root root 4096 Jan 21 18:51 attention_ocr\ndrwxr-xr-x  4 root root 4096 Jan 21 18:51 audioset\ndrwxr-xr-x  2 root root 4096 Jan 21 18:51 autoaugment\ndrwxr-xr-x  4 root root 4096 Jan 21 18:51 cognitive_planning\ndrwxr-xr-x  7 root root 4096 Jan 21 18:51 cvt_text\ndrwxr-xr-x  3 root root 4096 Jan 21 18:51 deep_speech\ndrwxr-xr-x  9 root root 4096 Jan 21 18:51 deeplab\ndrwxr-xr-x  3 root root 4096 Jan 21 18:51 delf\ndrwxr-xr-x  8 root root 4096 Jan 21 18:51 efficient-hrl\ndrwxr-xr-x  3 root root 4096 Jan 21 18:51 lfads\ndrwxr-xr-x 13 root root 4096 Jan 21 18:51 lstm_object_detection\ndrwxr-xr-x  2 root root 4096 Jan 21 18:51 marco\ndrwxr-xr-x  2 root root 4096 Jan 21 18:51 nst_blogpost\ndrwxr-xr-x 29 root root 4096 Jan 21 18:52 object_detection\ndrwxr-xr-x  2 root root 4096 Jan 21 18:51 pcl_rl\ndrwxr-xr-x  2 root root 4096 Jan 21 18:51 rebar\ndrwxr-xr-x 11 root root 4096 Jan 21 18:51 seq_flow_lite\ndrwxr-xr-x  7 root root 4096 Jan 21 18:51 slim\ndrwxr-xr-x  5 root root 4096 Jan 21 18:51 vid2depth\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\n\n#input_pattern='/kaggle/input/tensorflow-tfrecords-demystified/ArTaxOr-????1-of-00050.tfrecord;/kaggle/input/tensorflow-tfrecords-demystified/ArTaxOr-????7-of-00050.tfrecord'\n#input_files = tf.io.gfile.glob(input_pattern)\n# data_set = tf.data.TFRecordDataset('/kaggle/working/prog_outputs/tf-records/train.record')\n# records_n = sum(1 for record in data_set)\nrecords_n = 70 # takes a long time to run this, so cheating here\nprint(\"records_n = {}\".format(records_n))","execution_count":35,"outputs":[{"output_type":"stream","text":"records_n = 70\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\n\nos.environ['DATA_PATH']='/kaggle/working/prog_outputs/tf-records'       \nos.environ['MODEL_PATH']='object_detection/faster_rcnn_resnet50_fgvc_2018_07_19'","execution_count":36,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile 'object_detection/faster_rcnn_resnet50_fgvc_2018_07_19/sudoku.config'\nmodel {\n  faster_rcnn {\n    num_classes: 1 # sudoku has 1 class currently\n    image_resizer {\n      keep_aspect_ratio_resizer {\n        min_dimension: 600\n        max_dimension: 1024\n      }\n    }\n    feature_extractor {\n      type: 'faster_rcnn_resnet50'\n      first_stage_features_stride: 16\n    }\n    first_stage_anchor_generator {\n      grid_anchor_generator {\n        scales: [0.25, 0.5, 1.0, 2.0]\n        aspect_ratios: [0.5, 1.0, 2.0]\n        height_stride: 16\n        width_stride: 16\n      }\n    }\n    first_stage_box_predictor_conv_hyperparams {\n      op: CONV\n      regularizer {\n        l2_regularizer {\n          weight: 0.0\n        }\n      }\n      initializer {\n        truncated_normal_initializer {\n          stddev: 0.01\n        }\n      }\n    }\n    first_stage_nms_score_threshold: 0.0\n    first_stage_nms_iou_threshold: 0.7\n    first_stage_max_proposals: 300\n    first_stage_localization_loss_weight: 2.0\n    first_stage_objectness_loss_weight: 1.0\n    initial_crop_size: 14\n    maxpool_kernel_size: 2\n    maxpool_stride: 2\n    second_stage_batch_size: 32\n    second_stage_box_predictor {\n      mask_rcnn_box_predictor {\n        use_dropout: false\n        dropout_keep_probability: 1.0\n        fc_hyperparams {\n          op: FC\n          regularizer {\n            l2_regularizer {\n              weight: 0.0\n            }\n          }\n          initializer {\n            variance_scaling_initializer {\n              factor: 1.0\n              uniform: true\n              mode: FAN_AVG\n            }\n          }\n        }\n      }\n    }\n    second_stage_post_processing {\n      batch_non_max_suppression {\n        score_threshold: 0.0\n        iou_threshold: 0.6\n        max_detections_per_class: 50\n        max_total_detections: 100\n      }\n      score_converter: SOFTMAX\n    }\n    second_stage_localization_loss_weight: 2.0\n    second_stage_classification_loss_weight: 1.0\n  }\n}\n\ntrain_config: {\n  batch_size: 1\n  num_steps: 4000000\n  optimizer {\n    momentum_optimizer: {\n      learning_rate: {\n        manual_step_learning_rate {\n          initial_learning_rate: 0.0002\n          schedule {\n            step: 20000\n            learning_rate: .00002\n          }\n          schedule {\n            step: 50000\n            learning_rate: .000002\n          }\n        }\n      }\n      momentum_optimizer_value: 0.9\n    }\n    use_moving_average: false\n  }\n  gradient_clipping_by_norm: 10.0\n  fine_tune_checkpoint: \"/kaggle/models/research/object_detection/faster_rcnn_resnet50_fgvc_2018_07_19/model.ckpt\"\n  from_detection_checkpoint: true\n  load_all_detection_checkpoint_vars: true\n  data_augmentation_options {\n    random_horizontal_flip {\n    }\n  }\n}\n\ntrain_input_reader: {\n  label_map_path: \"/kaggle/working/prog_outputs/label_map.pbtxt\"\n  tf_record_input_reader {\n    input_path: \"/kaggle/working/prog_outputs/tf-records/train.record\"\n  }\n}\n\neval_config: {\n  metrics_set: \"pascal_voc_detection_metrics\"\n  #use_moving_averages: false\n  num_examples: 70\n}\n\neval_input_reader: {\n  label_map_path: \"/kaggle/working/prog_outputs/label_map.pbtxt\"\n  shuffle: false\n  num_readers: 1\n  tf_record_input_reader {\n    input_path: \"/kaggle/working/prog_outputs/tf-records/test.record\"\n  }\n}","execution_count":46,"outputs":[{"output_type":"stream","text":"Overwriting object_detection/faster_rcnn_resnet50_fgvc_2018_07_19/sudoku.config\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pwd","execution_count":47,"outputs":[{"output_type":"stream","text":"/kaggle/models/research\r\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Note! Tensorboard only works in editor mode (kernel running), so we will not be using it here.\n#%load_ext tensorboard\n#%tensorboard --logdir=object_detection/faster_rcnn_resnet50_fgvc_2018_07_19","execution_count":39,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"20000 steps take about 2h to run. Training will output large amounts of text, and once things are working it is better to dump it to a file rather than having to scroll down past thousands of lines."},{"metadata":{"trusted":true},"cell_type":"code","source":"old_stdout = sys.stdout\nsys.stdout = open('/kaggle/working/train.log', 'w')\n!python object_detection/model_main.py \\\n    --pipeline_config_path=object_detection/faster_rcnn_resnet50_fgvc_2018_07_19/sudoku.config \\\n    --model_dir=object_detection/faster_rcnn_resnet50_fgvc_2018_07_19 \\\n    --num_train_steps=1000 \\\n    --sample_1_of_n_eval_examples=1 \\\n    --alsologtostderr=False\nsys.stdout = old_stdout","execution_count":48,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Export the trained model to working directory using the supplied script."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture cap_out --no-stderr\n!mkdir /kaggle/working/trained\n!python object_detection/export_inference_graph.py \\\n    --input_type image_tensor \\\n    --pipeline_config_path object_detection/faster_rcnn_resnet50_fgvc_2018_07_19/sudoku.config \\\n    --trained_checkpoint_prefix object_detection/faster_rcnn_resnet50_fgvc_2018_07_19/model.ckpt-1000 \\\n    --output_directory /kaggle/working/trained","execution_count":49,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls -al /kaggle/working/trained","execution_count":50,"outputs":[{"output_type":"stream","text":"total 8\r\ndrwxr-xr-x 2 root root 4096 Jan 21 18:56 .\r\ndrwxr-xr-x 8 root root 4096 Jan 21 18:56 ..\r\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Zip it for easy download."},{"metadata":{"trusted":true},"cell_type":"code","source":"!tar -cvzf /kaggle/working/trained_model.tar /kaggle/working/trained\n!gzip /kaggle/working/trained_model.tar","execution_count":37,"outputs":[{"output_type":"stream","text":"tar: Removing leading `/' from member names\r\n/kaggle/working/trained/\r\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Let's check precision vs. training steps by parsing data from the log file."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --user parse -q\nfrom parse import *\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nwith open('/kaggle/working/train.log', 'r') as f:\n    data=f.read()","execution_count":51,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":52,"outputs":[{"output_type":"execute_result","execution_count":52,"data":{"text/plain":"'/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or \\'1type\\' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \\'(1,)type\\'.\\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\\n/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or \\'1type\\' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \\'(1,)type\\'.\\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\\n/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or \\'1type\\' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \\'(1,)type\\'.\\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\\n/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or \\'1type\\' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \\'(1,)type\\'.\\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\\n/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or \\'1type\\' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \\'(1,)type\\'.\\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\\n/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or \\'1type\\' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \\'(1,)type\\'.\\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\\n/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or \\'1type\\' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \\'(1,)type\\'.\\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\\n/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or \\'1type\\' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \\'(1,)type\\'.\\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\\n/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or \\'1type\\' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \\'(1,)type\\'.\\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\\n/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or \\'1type\\' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \\'(1,)type\\'.\\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\\n/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or \\'1type\\' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \\'(1,)type\\'.\\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\\n/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or \\'1type\\' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \\'(1,)type\\'.\\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\\nW0121 18:57:46.618700 139901282383232 model_lib.py:793] Forced number of epochs for all eval validations to be 1.\\nI0121 18:57:46.618942 139901282383232 config_util.py:552] Maybe overwriting train_steps: 1000\\nI0121 18:57:46.619084 139901282383232 config_util.py:552] Maybe overwriting use_bfloat16: False\\nI0121 18:57:46.619189 139901282383232 config_util.py:552] Maybe overwriting sample_1_of_n_eval_examples: 1\\nI0121 18:57:46.619315 139901282383232 config_util.py:552] Maybe overwriting eval_num_epochs: 1\\nW0121 18:57:46.619446 139901282383232 model_lib.py:809] Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\\nI0121 18:57:46.619577 139901282383232 model_lib.py:846] create_estimator_and_inputs: use_tpu False, export_to_tpu None\\nI0121 18:57:46.620168 139901282383232 estimator.py:209] Using config: {\\'_model_dir\\': \\'object_detection/faster_rcnn_resnet50_fgvc_2018_07_19\\', \\'_tf_random_seed\\': None, \\'_save_summary_steps\\': 100, \\'_save_checkpoints_steps\\': None, \\'_save_checkpoints_secs\\': 600, \\'_session_config\\': allow_soft_placement: true\\ngraph_options {\\n  rewrite_options {\\n    meta_optimizer_iterations: ONE\\n  }\\n}\\n, \\'_keep_checkpoint_max\\': 5, \\'_keep_checkpoint_every_n_hours\\': 10000, \\'_log_step_count_steps\\': 100, \\'_train_distribute\\': None, \\'_device_fn\\': None, \\'_protocol\\': None, \\'_eval_distribute\\': None, \\'_experimental_distribute\\': None, \\'_experimental_max_worker_delay_secs\\': None, \\'_service\\': None, \\'_cluster_spec\\': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f3cce9b8b38>, \\'_task_type\\': \\'worker\\', \\'_task_id\\': 0, \\'_global_id_in_cluster\\': 0, \\'_master\\': \\'\\', \\'_evaluation_master\\': \\'\\', \\'_is_chief\\': True, \\'_num_ps_replicas\\': 0, \\'_num_worker_replicas\\': 1}\\nW0121 18:57:46.621038 139901282383232 model_fn.py:630] Estimator\\'s model_fn (<function create_model_fn.<locals>.model_fn at 0x7f3cce9bae18>) includes params argument, but params are not passed to Estimator.\\nI0121 18:57:46.621961 139901282383232 estimator_training.py:186] Not using Distribute Coordinator.\\nI0121 18:57:46.622218 139901282383232 training.py:612] Running training and evaluation locally (non-distributed).\\nI0121 18:57:46.622587 139901282383232 training.py:700] Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\\nW0121 18:57:46.628298 139901282383232 deprecation.py:323] From /root/.local/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\\nInstructions for updating:\\nUse Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\\nTraceback (most recent call last):\\n  File \"object_detection/model_main.py\", line 108, in <module>\\n    tf.app.run()\\n  File \"/root/.local/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40, in run\\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\\n  File \"/opt/conda/lib/python3.6/site-packages/absl/app.py\", line 299, in run\\n    _run_main(main, args)\\n  File \"/opt/conda/lib/python3.6/site-packages/absl/app.py\", line 250, in _run_main\\n    sys.exit(main(argv))\\n  File \"object_detection/model_main.py\", line 104, in main\\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])\\n  File \"/root/.local/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\", line 473, in train_and_evaluate\\n    return executor.run()\\n  File \"/root/.local/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\", line 613, in run\\n    return self.run_local()\\n  File \"/root/.local/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\", line 714, in run_local\\n    saving_listeners=saving_listeners)\\n  File \"/root/.local/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 367, in train\\n    loss = self._train_model(input_fn, hooks, saving_listeners)\\n  File \"/root/.local/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1158, in _train_model\\n    return self._train_model_default(input_fn, hooks, saving_listeners)\\n  File \"/root/.local/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1185, in _train_model_default\\n    input_fn, ModeKeys.TRAIN))\\n  File \"/root/.local/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1022, in _get_features_and_labels_from_input_fn\\n    self._call_input_fn(input_fn, mode))\\n  File \"/root/.local/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1113, in _call_input_fn\\n    return input_fn(**kwargs)\\n  File \"/kaggle/models/research/object_detection/inputs.py\", line 735, in _train_input_fn\\n    params=params)\\n  File \"/kaggle/models/research/object_detection/inputs.py\", line 875, in train_input\\n    reduce_to_frame_fn=reduce_to_frame_fn)\\n  File \"/kaggle/models/research/object_detection/builders/dataset_builder.py\", line 195, in build\\n    decoder = decoder_builder.build(input_reader_config)\\n  File \"/kaggle/models/research/object_detection/builders/decoder_builder.py\", line 63, in build\\n    load_track_id=input_reader_config.load_track_id)\\n  File \"/kaggle/models/research/object_detection/data_decoders/tf_example_decoder.py\", line 393, in __init__\\n    default_value=\\'\\'),\\n  File \"/kaggle/models/research/object_detection/data_decoders/tf_example_decoder.py\", line 89, in __init__\\n    label_map_proto_file, use_display_name=False)\\n  File \"/kaggle/models/research/object_detection/utils/label_map_util.py\", line 201, in get_label_map_dict\\n    label_map = load_labelmap(label_map_path_or_proto)\\n  File \"/kaggle/models/research/object_detection/utils/label_map_util.py\", line 168, in load_labelmap\\n    label_map_string = fid.read()\\n  File \"/root/.local/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py\", line 122, in read\\n    self._preread_check()\\n  File \"/root/.local/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py\", line 84, in _preread_check\\n    compat.as_bytes(self.__name), 1024 * 512)\\ntensorflow.python.framework.errors_impl.NotFoundError: /kaggle/working/prog_outputs/label.pbtxt; No such file or directory\\n'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss=[]\nfor r in findall(\"Loss/RPNLoss/localization_loss = {:f}\", data):\n    loss.append(r[0])\nmAP=[]\nfor r in findall(\"/mAP@0.5IOU = {:f}\", data):\n    mAP.append(r[0])\nstep=[]\nfor r in findall(\"global_step = {:d}\", data):\n    step.append(r[0])\nplt.figure(figsize=(16, 8))\nplt.plot(step,mAP)\nplt.xlabel('Global step')\nplt.legend(['mAP@0.5IOU']);","execution_count":39,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1152x576 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"Let's have a look at precision for each class:"},{"metadata":{"trusted":true},"cell_type":"code","source":"Sudoku=[]\nfor r in findall(\"AP@0.5IOU/Sudoku = {:f}\", data):\n    Sudoku.append(r[0])\n# APColeoptera=[]\n# for r in findall(\"AP@0.5IOU/Coleoptera = {:f}\", data):\n#     APColeoptera.append(r[0])\n# APDiptera=[]\n# for r in findall(\"AP@0.5IOU/Diptera = {:f}\", data):\n#     APDiptera.append(r[0])\n# APHemiptera=[]\n# for r in findall(\"AP@0.5IOU/Hemiptera = {:f}\", data):\n#     APHemiptera.append(r[0])\n# APHymenoptera=[]\n# for r in findall(\"AP@0.5IOU/Hymenoptera = {:f}\", data):\n#     APHymenoptera.append(r[0])\n# APLepidoptera=[]\n# for r in findall(\"AP@0.5IOU/Lepidoptera = {:f}\", data):\n#     APLepidoptera.append(r[0])\n# APOdonata=[]\n# for r in findall(\"AP@0.5IOU/Odonata = {:f}\", data):\n#     APOdonata.append(r[0])\nplt.figure(figsize=(16, 8))\nplt.plot(step,Sudoku)\n# plt.plot(step,APColeoptera)\n# plt.plot(step,APDiptera)\n# plt.plot(step,APHemiptera)\n# plt.plot(step,APHymenoptera)\n# plt.plot(step,APLepidoptera)\n# plt.plot(step,APOdonata)\nplt.xlabel('Global step')\n# plt.legend(['AP Araneae', 'AP Coleoptera', 'AP Diptera', 'AP Hemiptera', 'AP Hymenoptera', 'AP Lepidoptera', 'AP Odonata']);\nplt.legend(['Sudoku']);","execution_count":41,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1152x576 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"## Prediction (detections)\nThe easiest way to make predictions (or detections) with the trained model is to use the API supplied script `infer_detections.py`, which expects images in a TFRecord file. Note that this script is difficult on Windows machines. We will make predictions on the [ArTaxOr TestSet](https://www.kaggle.com/mistag/arthropod-taxonomy-orders-object-detection-testset). The [starter kernel](https://www.kaggle.com/mistag/starter-arthropod-taxonomy-orders-testset) outputs a TFRecord file, so we can simply link to that. The detections are output in a separate TFRecord file, which we will process further down."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture cap_out --no-stderr\n!python object_detection/inference/infer_detections.py \\\n  --input_tfrecord_paths=/kaggle/working/prog_outputs/tf-records/test.record \\\n  --output_tfrecord_path=/kaggle/working/sudoku_detections.record \\\n  --inference_graph=/kaggle/working/trained/frozen_inference_graph.pb \\\n  --discard_image_pixels","execution_count":45,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls /kaggle/working/","execution_count":51,"outputs":[{"output_type":"stream","text":"__notebook_source__.ipynb  include\t readme.txt  trained\r\nbin\t\t\t   prog_outputs  train.log   trained_model.tar.gz\r\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"First, we'll import pickled annotation data from the [ArtAxOr TestSet Starter notebook](https://www.kaggle.com/mistag/starter-arthropod-taxonomy-orders-testset). Then we read in the TFRecord with the detections, and create a Pandas frame with the detected bounding boxes."},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%capture\n# import pandas as pd\n# import tensorflow as tf\n\n# labels=pd.read_pickle('/kaggle/input/starter-arthropod-taxonomy-orders-testset/testset_labels.pkl')\n# df=pd.read_pickle('/kaggle/input/starter-arthropod-taxonomy-orders-testset/testset_filelist.pkl')\n# anno=pd.read_pickle('/kaggle/input/starter-arthropod-taxonomy-orders-testset/testset_objects.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pdf=pd.DataFrame(columns=['score', 'label_idx', 'left', 'top', 'right', 'bottom', 'by', 'filename'])\nexample = tf.train.Example()\nfor record in tf.compat.v1.io.tf_record_iterator('/kaggle/working/sudoku_detections.record'):\n    example.ParseFromString(record)\n    f = example.features.feature\n    score = f['image/detection/score'].float_list.value\n    score = [x for x in score if x >= 0.60]\n    l = len(score)\n    pdf=pdf.append({'score': score,\n                    'label_idx': f['image/detection/label'].int64_list.value[:l],\n                    'left': f['image/detection/bbox/xmin'].float_list.value[:l],\n                    'top': f['image/detection/bbox/ymin'].float_list.value[:l],\n                    'right': f['image/detection/bbox/xmax'].float_list.value[:l],\n                    'bottom': f['image/detection/bbox/ymax'].float_list.value[:l],\n                    'by': f['image/by'].bytes_list.value[0].decode(),\n                    'filename': f['image/filename'].bytes_list.value[0].decode()}, ignore_index=True)","execution_count":44,"outputs":[{"output_type":"error","ename":"NotFoundError","evalue":"/kaggle/working/sudoku_detections.tfrecord; No such file or directory","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-44-c94a9b9d7418>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'label_idx'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'left'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'top'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'right'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bottom'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'by'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'filename'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mrecord\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_record_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/working/sudoku_detections.tfrecord'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mexample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParseFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/lib/io/tf_record.py\u001b[0m in \u001b[0;36mtf_record_iterator\u001b[0;34m(path, options)\u001b[0m\n\u001b[1;32m    172\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     reader = pywrap_tensorflow.PyRecordReader_New(\n\u001b[0;32m--> 174\u001b[0;31m         compat.as_bytes(path), 0, compat.as_bytes(compression_type), status)\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mreader\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    549\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotFoundError\u001b[0m: /kaggle/working/sudoku_detections.tfrecord; No such file or directory"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pdf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then we define a few helper functions for plotting the test images and bounding boxes."},{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install --user python-resize-image -q","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from PIL import Image, ImageFont, ImageDraw\n# from resizeimage import resizeimage\n# import numpy as np\n\n# TSET_PATH = '/kaggle/input/arthropod-taxonomy-orders-object-detection-testset/ArTaxOr_TestSet/'\n\n# #fontname = 'C:/Windows/fonts/micross.ttf' # Windows\n# fontname = '/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf' # Linux\n# font = ImageFont.truetype(fontname, 20) if os.path.isfile(fontname) else ImageFont.load_default()\n\n# def resize_image(file, width, height, stretch=False):\n#     with Image.open(file) as im:\n#         img = im.resize((width, height)) if stretch else resizeimage.resize_contain(im, [width, height])\n#     img=img.convert(\"RGB\")    \n#     return img\n\n# #draw boundary box\n# def bbox(img, xmin, ymin, xmax, ymax, color, width, label, score):\n#     draw = ImageDraw.Draw(img)\n#     xres, yres = img.size[0], img.size[1]\n#     box = np.multiply([xmin, ymin, xmax, ymax], [xres, yres, xres, yres]).astype(int).tolist()\n#     txt = \" {}: {}%\" if score >= 0. else \" {}\"\n#     txt = txt.format(label, round(score, 1))\n#     ts = draw.textsize(txt, font=font)\n#     draw.rectangle(box, outline=color, width=width)\n#     if len(label) > 0:\n#         if box[1] >= ts[1]+3:\n#             xsmin, ysmin = box[0], box[1]-ts[1]-3\n#             xsmax, ysmax = box[0]+ts[0]+2, box[1]\n#         else:\n#             xsmin, ysmin = box[0], box[3]\n#             xsmax, ysmax = box[0]+ts[0]+2, box[3]+ts[1]+1\n#         draw.rectangle([xsmin, ysmin, xsmax, ysmax], fill=color)\n#         draw.text((xsmin, ysmin), txt, font=font, fill='white')\n    \n# #prediction\n# def plot_img_pred(img, xres, yres, axes, scores, xmin, ymin, xmax, ymax, classes, title, by=''):\n#     wscale = min(1,xres/yres)\n#     hscale = min(1,yres/xres)\n#     for i in range(len(scores)):\n#         if scores[i]> 0.5 and classes[i]>0:\n#             label = labels.name.iloc[int(classes[i]-1)]\n#             color=labels.color.iloc[int(classes[i]-1)]\n#             width, height = xmax[i]-xmin[i], ymax[i]-ymin[i]\n#             xcenter, ycenter = xmin[i] + width/2., ymin[i] + height/2.\n#             sxmin = .5+(xcenter-.5)*wscale-.5*wscale*width\n#             symin = .5+(ycenter-.5)*hscale-.5*hscale*height\n#             sxmax = .5+(xcenter-.5)*wscale+.5*wscale*width\n#             symax = .5+(ycenter-.5)*hscale+.5*hscale*height\n#             bbox(img, sxmin, symin, sxmax, symax, color, 2, label, 100*scores[i])\n#     plt.setp(axes, xticks=[], yticks=[])\n#     axes.set_title(title) if by == '' else axes.set_title(title+'\\n'+by)\n#     plt.imshow(img)\n\n# #ground truth\n# def plot_img_gt(img, axes, boxes, stretch, title, by=''):\n#     wscale = 1. if stretch else min(1,boxes.xres.iloc[0]/boxes.yres.iloc[0])\n#     hscale = 1. if stretch else min(1,boxes.yres.iloc[0]/boxes.xres.iloc[0])\n#     for i in range(len(boxes)):\n#         label = boxes.label.iloc[i]\n#         color=labels.color.iloc[boxes.label_idx.iloc[i]]\n#         xmin = .5+(boxes.xcenter.iloc[i]-.5)*wscale-.5*wscale*boxes.width.iloc[i]\n#         ymin = .5+(boxes.ycenter.iloc[i]-.5)*hscale-.5*hscale*boxes.height.iloc[i]\n#         xmax = .5+(boxes.xcenter.iloc[i]-.5)*wscale+.5*wscale*boxes.width.iloc[i]\n#         ymax = .5+(boxes.ycenter.iloc[i]-.5)*hscale+.5*hscale*boxes.height.iloc[i]\n#         bbox(img, xmin, ymin, xmax, ymax, color, 2, label, -1)\n#     plt.setp(axes, xticks=[], yticks=[])\n#     axes.set_title(title) if by == '' else axes.set_title(title+'\\n'+by)\n#     plt.imshow(img)\n\n# def pred_batch(idx):\n#     if idx + 2 < len(pdf):\n#         rows = 3\n#     else:\n#         rows = len(pdf) - idx\n#     fig = plt.figure(figsize=(16,rows*8))\n#     for i in range(rows):\n#         img = resize_image(TSET_PATH+'positives/'+pdf.filename.iloc[i+idx], 512, 512, False)\n#         by = pdf.by.iloc[i+idx]\n#         axes = fig.add_subplot(rows, 2, 1+i*2)\n#         boxes = anno[anno.id == df.id.iloc[i+idx]][['label', 'label_idx', 'xres', 'yres', 'xcenter', 'ycenter', 'width', 'height']]\n#         plot_img_gt(img, axes, boxes, False, 'Ground truth', by)\n#         img = resize_image(TSET_PATH+'positives/'+pdf.filename.iloc[i+idx], 512, 512, False)\n#         axes = fig.add_subplot(rows, 2, 2+i*2)\n#         plot_img_pred(img, boxes.xres.iloc[0], boxes.yres.iloc[0], axes, pdf.score[i+idx], pdf.left[i+idx], pdf.top[i+idx], \n#                       pdf.right[i+idx], pdf.bottom[i+idx],\n#                       pdf.label_idx[i+idx], 'Detections', '')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally we can show some images, with ground truth to the left and detections to the right. Some detections are overlapping, and additional non-max suppression seems to be needed here."},{"metadata":{"trusted":true},"cell_type":"code","source":"# pred_batch(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pred_batch(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pred_batch(6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pred_batch(9)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pred_batch(12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pred_batch(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pred_batch(18)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Summary\nAny object detection framework that has all files located in a directory called `research` should ring a few alarm bells when it comes to expectations of a slick user experience. However, we have seen that a few tweaks are all that is needed to get going with the TensorFlow Object Detection API. What about other options for object detection? PyTorch Detectron2 is the only other framework that has a pretrained model zoo, but currently it does not run on Kaggle (and no Windows support). TensorFlow Hub has several pre-trained models, and otherwise one would have to engage in detail implementation of models like YOLO etc. What we really need is to get object detection from research level and into mainstream."}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}