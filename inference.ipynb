{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAALHElEQVR4nO3dQYic93nH8e+vrqyAk4JV10Z1TJMGH2oKVcqiFlyKi2ni+CLnkBIdggsG5RBDAjnUpIf4aEqT0EMJKLWIWlKHQGKsg6kjRMDkYrw2ri1Xbe0aNVEkpAYf4rRUlp2nhx2VjbyrXc07M+9on+8Hlpl5Z3bn8eCv3tn5z+ybqkLSzvcrYw8gaTGMXWrC2KUmjF1qwtilJn51kXd2Y3bX+7hpkXcptfK//Ddv18VsdN2g2JPcB/wNcAPwd1X12NVu/z5u4g9y75C7lHQVz9WJTa+b+ml8khuAvwU+AdwFHExy17Q/T9J8DfmdfT/welW9UVVvA98GDsxmLEmzNiT224Efr7t8ZrLtlyQ5lGQ1yeolLg64O0lDDIl9oxcB3vPe26o6XFUrVbWyi90D7k7SEENiPwPcse7yB4Gzw8aRNC9DYn8euDPJh5PcCHwaODabsSTN2tRLb1X1TpKHgWdYW3o7UlWvDhnmmbMvDfl2qb39H/+fTa8btM5eVU8DTw/5GZIWw7fLSk0Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71MSgQzYnOQ28BbwLvFNVK7MYStLsDYp94k+q6qcz+DmS5sin8VITQ2Mv4PtJXkhyaKMbJDmUZDXJ6iUuDrw7SdMa+jT+7qo6m+RW4HiSf62qZ9ffoKoOA4cBfi17auD9SZrSoD17VZ2dnF4AngT2z2IoSbM3dexJbkrygcvngY8BJ2c1mKTZGvI0/jbgySSXf84/VtU/zWQqXTc+/pv7pv7eZ86+NMNJtJWpY6+qN4Dfm+EskubIpTepCWOXmjB2qQljl5owdqmJWXwQRktsyNLYvG01m0tzs+WeXWrC2KUmjF1qwtilJoxdasLYpSaMXWrCdfYdYMy19CFr4cv8HoCdyD271ISxS00Yu9SEsUtNGLvUhLFLTRi71ITr7NeBea5Hj/mZcT+vvlju2aUmjF1qwtilJoxdasLYpSaMXWrC2KUmXGff4VzL1mVb7tmTHElyIcnJddv2JDme5LXJ6c3zHVPSUNt5Gv9N4L4rtj0CnKiqO4ETk8uSltiWsVfVs8CbV2w+ABydnD8KPDDjuSTN2LQv0N1WVecAJqe3bnbDJIeSrCZZvcTFKe9O0lBzfzW+qg5X1UpVrexi97zvTtImpo39fJK9AJPTC7MbSdI8TBv7MeDByfkHgadmM46kedlynT3JE8A9wC1JzgBfBh4DvpPkIeBHwKfmOeRO599P1yJsGXtVHdzkqntnPIukOfLtslITxi41YexSE8YuNWHsUhN+xHUHuF4Pm+zHbxfLPbvUhLFLTRi71ISxS00Yu9SEsUtNGLvUhOvsO8D1+hHZreZ2HX623LNLTRi71ISxS00Yu9SEsUtNGLvUhLFLTbjOvsONvVY95D0ArsPPlnt2qQljl5owdqkJY5eaMHapCWOXmjB2qQnX2XeAZV5vvtpsQz+H7zr8tdlyz57kSJILSU6u2/Zokp8keWnydf98x5Q01Haexn8TuG+D7V+rqn2Tr6dnO5akWdsy9qp6FnhzAbNImqMhL9A9nOTlydP8mze7UZJDSVaTrF7i4oC7kzTEtLF/HfgIsA84B3xlsxtW1eGqWqmqlV3snvLuJA01VexVdb6q3q2qXwDfAPbPdixJszZV7En2rrv4SeDkZreVtBy2XGdP8gRwD3BLkjPAl4F7kuwDCjgNfHaOM2qH2mod/Hr9e/jLasvYq+rgBpsfn8MskubIt8tKTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUhLFLTfinpHeAq30U1D+nrMvcs0tNGLvUhLFLTRi71ISxS00Yu9SEsUtNuM6+BLr+SeWh/12+h+DauGeXmjB2qQljl5owdqkJY5eaMHapCWOXmnCdfYcbey17p75H4Hq05Z49yR1JfpDkVJJXk3x+sn1PkuNJXpuc3jz/cSVNaztP498BvlhVvwP8IfC5JHcBjwAnqupO4MTksqQltWXsVXWuql6cnH8LOAXcDhwAjk5udhR4YF5DShruml6gS/Ih4KPAc8BtVXUO1v5BAG7d5HsOJVlNsnqJi8OmlTS1bcee5P3Ad4EvVNXPtvt9VXW4qlaqamUXu6eZUdIMbCv2JLtYC/1bVfW9yebzSfZOrt8LXJjPiJJmYcultyQBHgdOVdVX1111DHgQeGxy+tRcJtSoH4Edc+nMj7DO1nbW2e8GPgO8kuTyo/8l1iL/TpKHgB8Bn5rPiJJmYcvYq+qHQDa5+t7ZjiNpXny7rNSEsUtNGLvUhLFLTRi71IQfcd0BXI/Wdrhnl5owdqkJY5eaMHapCWOXmjB2qQljl5owdqkJY5eaMHapCWOXmjB2qQljl5owdqkJY5eaMHapCWOXmjB2qQljl5owdqkJY5eaMHapCWOXmtgy9iR3JPlBklNJXk3y+cn2R5P8JMlLk6/75z+upGlt5yAR7wBfrKoXk3wAeCHJ8cl1X6uqv57feJJmZTvHZz8HnJucfyvJKeD2eQ8mabau6Xf2JB8CPgo8N9n0cJKXkxxJcvMm33MoyWqS1UtcHDSspOltO/Yk7we+C3yhqn4GfB34CLCPtT3/Vzb6vqo6XFUrVbWyi90zGFnSNLYVe5JdrIX+rar6HkBVna+qd6vqF8A3gP3zG1PSUNt5NT7A48Cpqvrquu17193sk8DJ2Y8naVa282r83cBngFeSXD428JeAg0n2AQWcBj47lwklzcR2Xo3/IZANrnp69uNImhffQSc1YexSE8YuNWHsUhPGLjVh7FITxi41YexSE8YuNWHsUhPGLjVh7FITxi41YexSE6mqxd1Z8l/Af67bdAvw04UNcG2WdbZlnQucbVqznO23quo3NrpiobG/586T1apaGW2Aq1jW2ZZ1LnC2aS1qNp/GS00Yu9TE2LEfHvn+r2ZZZ1vWucDZprWQ2Ub9nV3S4oy9Z5e0IMYuNTFK7EnuS/JvSV5P8sgYM2wmyekkr0wOQ7068ixHklxIcnLdtj1Jjid5bXK64TH2RpptKQ7jfZXDjI/62I19+POF/86e5Abg34E/Bc4AzwMHq+pfFjrIJpKcBlaqavQ3YCT5Y+DnwN9X1e9Otv0V8GZVPTb5h/LmqvqLJZntUeDnYx/Ge3K0or3rDzMOPAD8OSM+dleZ689YwOM2xp59P/B6Vb1RVW8D3wYOjDDH0quqZ4E3r9h8ADg6OX+Utf9ZFm6T2ZZCVZ2rqhcn598CLh9mfNTH7ipzLcQYsd8O/Hjd5TMs1/HeC/h+kheSHBp7mA3cVlXnYO1/HuDWkee50paH8V6kKw4zvjSP3TSHPx9qjNg3OpTUMq3/3V1Vvw98Avjc5Omqtmdbh/FelA0OM74Upj38+VBjxH4GuGPd5Q8CZ0eYY0NVdXZyegF4kuU7FPX5y0fQnZxeGHme/7dMh/He6DDjLMFjN+bhz8eI/XngziQfTnIj8Gng2AhzvEeSmyYvnJDkJuBjLN+hqI8BD07OPwg8NeIsv2RZDuO92WHGGfmxG/3w51W18C/gftZekf8P4C/HmGGTuX4b+OfJ16tjzwY8wdrTukusPSN6CPh14ATw2uR0zxLN9g/AK8DLrIW1d6TZ/oi1Xw1fBl6afN0/9mN3lbkW8rj5dlmpCd9BJzVh7FITxi41YexSE8YuNWHsUhPGLjXxf3nNgsDhQN8iAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "reconstructed_model =  keras.models.load_model(\"digit_recognizer_best_model.h5\")\n",
    "test_image = Image.open('crop_img/crop_10.png')\n",
    "test_image_resized = test_image.resize((28,28))\n",
    "\n",
    "test_image_resized.save('crop_img/crop_1_resized.png')\n",
    "test_image_resized_2 = cv2.imread('crop_img/crop_1_resized.png')\n",
    "test_image_resized_gray = cv2.cvtColor(test_image_resized_2,cv2.COLOR_BGR2GRAY)\n",
    "ret, thresh = cv2.threshold(test_image_resized_gray,140,255,cv2.THRESH_BINARY)\n",
    "nonZero = cv2.countNonZero(thresh)\n",
    "plt.imshow(thresh)\n",
    "if nonZero/784 >0.9:\n",
    "    print('blank')\n",
    "else:\n",
    "    bitwise_thresh = cv2.bitwise_not(thresh)\n",
    "    np_img_3 = bitwise_thresh.reshape([-1,28, 28,1])\n",
    "    res = reconstructed_model.predict(np_img_3)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# for i in range(0,len(res)):\n",
    "a = np.argmax(res)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running predictions on OD model trained to predit sudoku bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import cv2\n",
    "\n",
    "from distutils.version import StrictVersion\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# This is needed since the notebook is stored in the object_detection folder.\n",
    "sys.path.append(\"..\")\n",
    "from object_detection.utils import ops as utils_ops\n",
    "\n",
    "# if StrictVersion(tf.__version__) < StrictVersion('1.9.0'):\n",
    "#   raise ImportError('Please upgrade your TensorFlow installation to v1.9.* or later!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is needed to display the images.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = r\"C:/Users/niharika.a.kumari/OneDrive - Accenture/My Files/Personal GitHub/Sudoku-HoughLine-OCR/trained/\"\n",
    "PATH_TO_FROZEN_GRAPH = MODEL_NAME + 'frozen_inference_graph.pb'\n",
    "PATH_TO_LABELS = MODEL_NAME + \"label_map.pbtxt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "  od_graph_def = tf.compat.v1.GraphDef()\n",
    "  with tf.compat.v2.io.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:\n",
    "    serialized_graph = fid.read()\n",
    "    od_graph_def.ParseFromString(serialized_graph)\n",
    "    tf.import_graph_def(od_graph_def, name='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(image):\n",
    "  (im_width, im_height) = image.size\n",
    "  return np.array(image.getdata()).reshape(\n",
    "      (im_height, im_width, 3)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the sake of simplicity we will use only 2 images:\n",
    "# image1.jpg\n",
    "# image2.jpg\n",
    "# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.\n",
    "PATH_TO_TEST_IMAGES_DIR = 'test_images'\n",
    "TEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR, 'Screenshot_{}.jpg'.format(i)) for i in range(1, 3) ]\n",
    "\n",
    "# Size, in inches, of the output images.\n",
    "IMAGE_SIZE = (12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(image, graph):\n",
    "  with graph.as_default():\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "      # Get handles to input and output tensors\n",
    "      ops = tf.compat.v1.get_default_graph().get_operations()\n",
    "      all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
    "      tensor_dict = {}\n",
    "      for key in [\n",
    "          'num_detections', 'detection_boxes', 'detection_scores',\n",
    "          'detection_classes', 'detection_masks'\n",
    "      ]:\n",
    "        tensor_name = key + ':0'\n",
    "        if tensor_name in all_tensor_names:\n",
    "          tensor_dict[key] = tf.compat.v1.get_default_graph().get_tensor_by_name(\n",
    "              tensor_name)\n",
    "      if 'detection_masks' in tensor_dict:\n",
    "        # The following processing is only for single image\n",
    "        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
    "        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
    "        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
    "        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
    "        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
    "        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(\n",
    "            tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
    "        # Follow the convention by adding back the batch dimension\n",
    "        tensor_dict['detection_masks'] = tf.expand_dims(\n",
    "            detection_masks_reframed, 0)\n",
    "      image_tensor = tf.compat.v1.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "      # Run inference\n",
    "      output_dict = sess.run(tensor_dict,\n",
    "                             feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
    "\n",
    "      # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "      output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
    "      output_dict['detection_classes'] = output_dict[\n",
    "          'detection_classes'][0].astype(np.uint8)\n",
    "      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "      output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "      if 'detection_masks' in output_dict:\n",
    "        output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
    "  return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "for image_path in TEST_IMAGE_PATHS:\n",
    "  image = Image.open(image_path)\n",
    "  # the array based representation of the image will be used later in order to prepare the\n",
    "  # result image with boxes and labels on it.\n",
    "  image_np = load_image_into_numpy_array(image)\n",
    "  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "  image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "  # Actual detection.\n",
    "  output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
    "  # Visualization of the results of a detection.\n",
    "  vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "      image_np,\n",
    "      output_dict['detection_boxes'],\n",
    "      output_dict['detection_classes'],\n",
    "      output_dict['detection_scores'],\n",
    "      category_index,\n",
    "      instance_masks=output_dict.get('detection_masks'),\n",
    "      use_normalized_coordinates=True,\n",
    "      line_thickness=8)\n",
    "  cv2.imwrite('pred_images/predicted_image{}.png'.format(i),image_np)\n",
    "  i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_detections': 50,\n",
       " 'detection_boxes': array([[0.10582907, 0.07347202, 0.58970803, 1.        ],\n",
       "        [0.20157982, 0.03127602, 0.6174757 , 0.7138194 ],\n",
       "        [0.28258252, 0.        , 0.56928164, 1.        ],\n",
       "        [0.13482077, 0.        , 0.48555332, 0.9666645 ],\n",
       "        [0.22672655, 0.29185197, 0.6512362 , 1.        ],\n",
       "        [0.14757074, 0.34482512, 0.5747553 , 0.9708366 ],\n",
       "        [0.03336324, 0.        , 0.35304147, 0.9581921 ],\n",
       "        [0.33142924, 0.        , 0.6651375 , 1.        ],\n",
       "        [0.        , 0.19993274, 0.36720717, 0.9907725 ],\n",
       "        [0.09780094, 0.07108477, 0.6196096 , 0.44826165],\n",
       "        [0.        , 0.06703428, 0.29984462, 0.82566464],\n",
       "        [0.242776  , 0.08337714, 0.72240174, 0.9191911 ],\n",
       "        [0.26322487, 0.5694347 , 0.81473696, 0.9849351 ],\n",
       "        [0.15965858, 0.        , 0.43909177, 0.6790578 ],\n",
       "        [0.12776901, 0.05033915, 0.3761052 , 0.58815366],\n",
       "        [0.01118207, 0.        , 0.21060552, 0.70760226],\n",
       "        [0.09997851, 0.12437672, 0.4159888 , 1.        ],\n",
       "        [0.13349782, 0.        , 0.51814246, 0.5455601 ],\n",
       "        [0.4174463 , 0.        , 0.80586374, 0.93272066],\n",
       "        [0.3406251 , 0.30558404, 0.7850657 , 0.99814844],\n",
       "        [0.01836045, 0.06135772, 0.304721  , 0.5577097 ],\n",
       "        [0.35319084, 0.        , 0.5438741 , 0.9424222 ],\n",
       "        [0.11582579, 0.40634868, 0.3825088 , 1.        ],\n",
       "        [0.        , 0.24597634, 0.2120443 , 0.7955756 ],\n",
       "        [0.30339932, 0.11260606, 0.67292666, 0.66571563],\n",
       "        [0.42715663, 0.        , 0.58729213, 0.9513102 ],\n",
       "        [0.06619936, 0.15454996, 0.28736052, 0.7194524 ],\n",
       "        [0.07130489, 0.04964089, 0.31003433, 0.39087898],\n",
       "        [0.        , 0.34023416, 0.19075456, 0.95271564],\n",
       "        [0.3818553 , 0.10432847, 0.61589247, 0.48623526],\n",
       "        [0.9299003 , 0.01144292, 1.        , 0.07006284],\n",
       "        [0.6001909 , 0.03797299, 0.8901919 , 0.9429794 ],\n",
       "        [0.3903454 , 0.09524272, 0.7984932 , 0.7182317 ],\n",
       "        [0.23020652, 0.        , 0.39682075, 0.51015335],\n",
       "        [0.23050496, 0.01081935, 0.7805363 , 0.5033271 ],\n",
       "        [0.49547362, 0.06717322, 0.6760912 , 0.9416409 ],\n",
       "        [0.44210306, 0.        , 0.7627014 , 0.5886105 ],\n",
       "        [0.03193571, 0.42482436, 0.2864715 , 0.9914459 ],\n",
       "        [0.03133951, 0.06989789, 0.21265484, 0.5131767 ],\n",
       "        [0.        , 0.02313566, 0.12976044, 0.5836563 ],\n",
       "        [0.07622972, 0.2085774 , 0.3036047 , 0.9584638 ],\n",
       "        [0.64644116, 0.08401515, 0.85953754, 0.8072214 ],\n",
       "        [0.9149696 , 0.92373335, 0.9973567 , 0.9903499 ],\n",
       "        [0.33668342, 0.2835791 , 0.5371164 , 0.9767446 ],\n",
       "        [0.5880136 , 0.2632111 , 0.83684736, 0.9684469 ],\n",
       "        [0.75742286, 0.00695832, 0.93504983, 1.        ],\n",
       "        [0.5999735 , 0.01781494, 0.8412263 , 0.6069228 ],\n",
       "        [0.07497869, 0.01226352, 0.21005075, 0.42704275],\n",
       "        [0.54123664, 0.2873288 , 0.7798133 , 1.        ],\n",
       "        [0.31000072, 0.44288725, 0.4730531 , 0.9713943 ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ]], dtype=float32),\n",
       " 'detection_scores': array([9.93404567e-01, 9.66256380e-01, 9.11645472e-01, 8.44086111e-01,\n",
       "        3.53843421e-01, 1.93068042e-01, 1.69463068e-01, 1.43187702e-01,\n",
       "        1.24467798e-01, 8.48096982e-02, 7.79799521e-02, 6.00765795e-02,\n",
       "        5.08528277e-02, 4.04439382e-02, 1.59444511e-02, 1.30020520e-02,\n",
       "        1.25788776e-02, 1.04054613e-02, 7.05137802e-03, 5.93189290e-03,\n",
       "        2.55149370e-03, 1.20104617e-03, 1.05012220e-03, 9.59827390e-04,\n",
       "        6.55830721e-04, 4.72950691e-04, 4.39163327e-04, 4.02455888e-04,\n",
       "        3.75340343e-04, 2.83240166e-04, 2.30001126e-04, 2.05684148e-04,\n",
       "        1.95317742e-04, 1.78598842e-04, 1.51017011e-04, 1.24214232e-04,\n",
       "        1.12077105e-04, 8.78694045e-05, 6.60827645e-05, 5.22046539e-05,\n",
       "        3.95594398e-05, 2.14014726e-05, 2.08896963e-05, 1.83626016e-05,\n",
       "        1.11852896e-05, 1.03968414e-05, 1.00608368e-05, 9.62825106e-06,\n",
       "        7.79049788e-06, 6.42337318e-06, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       dtype=float32),\n",
       " 'detection_classes': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=uint8)}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
